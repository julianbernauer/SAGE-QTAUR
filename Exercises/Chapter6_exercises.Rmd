---
title: "Chapter6_exercises"
author: "Anna Wohlmann"
date: "2025-01-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1) Go to the website polidoc and download manifestos of your choice in one language. Create a dfm and run a wordfish model.

Download the data of your choice and create a dfm: 
We chose manifestos from Austria from 2017 from polidoc.
```{r}
datadir <- "./data_exercises"

library(readtext)
library(quanteda) #read in necessary packages
manifestos_df<- readtext(paste0(datadir,"/austria_manifestos"), encoding= "UTF-8") #read in data from folder
country <- c("Austria", "Austria", "Austria", "Austria", "Austria") #add a country variable 
party <- c("Die Grünen", "KPÖ","SPÖ", "FPÖ", "NEOS") #party of manifesto variable - same order as in folder!
year <- c("2017", "2017", "2017", "2017", "2017") #year of manifesto variable
manifestos_df$country <- country 
manifestos_df$year <- year
manifestos_df$party <- party #add additional variables to data frame

corpus_manifestos <- corpus(manifestos_df) #turn dataframe into corpus
summary(corpus_manifestos) #print corpus summary

dfm_aus <-
  quanteda::dfm(corpus_manifestos %>%
                  quanteda::tokens(
                    remove_punct = TRUE,
                    remove_numbers = TRUE,
                    remove_symbols = TRUE,
                    remove_url = TRUE)) %>%
    quanteda::dfm_remove(stopwords("de")) %>% 
  quanteda::dfm_wordstem(language = "de")
head(dfm_aus)

```

Run Wordfish:
We need to rank two texts. Here, we make the justifiable statement that Die Grünen are left of FPÖ.
```{r}
library(quanteda.textmodels)
library(quanteda.textplots)
fish_aus <- textmodel_wordfish(dfm_aus, dir = c(1,4)) #1 = Die Grünen, 4 = FPÖ
summary(fish_aus, n = 5) 
```

You can visualize the results:
```{r} 
textplot_scale1d(fish_aus, doclabels=fish_aus$x@docvars$party) 
```


2) Replicate the code in the k-means section for the word2vec model on the data used here, or choose another text dataset of your choice. You will keep using this model for Exercise 2 of Chapter 7.

To correct your replication, you can check the chapter again. We do it here quickly:

```{r}
datadir <- "./data_exercises"
load(paste0(datadir, "\\supercorpus_unstemmed_V4.RData")) #load where you saved it
library(word2vec)
clean_doc <- txt_clean_word2vec(corpus_unstemmed[["documents"]][["texts"]], ascii = TRUE, alpha = TRUE, tolower = TRUE, trim = TRUE)
library(stopwords)
set.seed(45) #same results every run
model_w2v <- word2vec(clean_doc, stopwords = stopwords("en")) #run model

```
a)	Find out what the six nearest neighbours, meaning most similar words, are to the word “ideology”.
```{r }
set.seed(5)
predict(model_w2v, "ideology", type = "nearest", top_n = 6)

```
In this dataset, intolerance, dogmatism, obsession, and fanaticism are closely related to ideology.

b)	Choose your own word and find the words closest to it based on the speeches dataset.
```{r }
set.seed(5)
predict(model_w2v, "racism", type = "nearest", top_n = 1)

```
For racism, the nearest neighbor is xenophobia.

c)	Find the embedding vector for the word “speech”. Hint: word2vec has documentation that you can access online.

The documentation on CRAN shows that we can use the predict function again:
```{r }
??word2vec::predict
set.seed(5)
predict(model_w2v, "speech", type = "embedding")

```


3) Use the data from Exercise 1. Replicate the LDA analysis from this chapter, finding out how much the chosen parties talk about renewable energy and unemployment. You will keep using this model in Exercise 3 of Chapter 7.

In exercise 1, we worked on the whole document; here, we are interested in individual sentences. Therefore, we need to reshape the corpus and then create a dtm:

```{r}
corpus_sent <- corpus_reshape(corpus_manifestos, to = "sentences") #transform into sentence corpus

dfm_sent_aus <-
  quanteda::dfm(corpus_sent %>%
                  quanteda::tokens(
                    remove_punct = TRUE,
                    remove_numbers = TRUE,
                    remove_symbols = TRUE,
                    remove_url = TRUE)) %>%
    quanteda::dfm_remove(stopwords("de")) %>% 
  quanteda::dfm_wordstem(language = "de")
head(dfm_sent_aus)

```

For LDA, we need a DTM: 
```{r }
dtm_aus <- convert(dfm_sent_aus, to = "topicmodels")
```

Finding k:
We use the ldatuning package to decide on k.
```{r }
library("ldatuning") #install this package if you haven't yet, then use library
result_aus <- FindTopicsNumber(
  dtm_aus, #pass dtm
  topics = seq(from = 5, to = 100, by = 5), #calculated amount of topics
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), #the algorithms
  method = "Gibbs", #the sampling algorithm
  control = list(seed = 77), #same result if code is run again
  verbose = TRUE #so R tells you what it is working on right now
)

FindTopicsNumber_plot(result_aus)
```
Looking at the maximum of Griffiths, I choose 15 here.

Fit the LDA model:
We use the topicmodels package and the k from above.
```{r}
library(topicmodels) #load
set.seed(50)  #same result if you run the code again
model_aus <- topicmodels::LDA(dtm_aus, k = 15, method = "Gibbs", control = list(alpha = 0.5, burnin = 1000, iter =3000)) #burnin and iter will be improved in exercise 3 of chapter 7

```

Let's look at the terms to find renewable energy and unemployment:
```{r }
terms(model_aus, 20) 
```
Topic 15 contains "arbeitslos" meaning unemployed. Topic 10 discusses nature, but renewable energy is not highly influential on the topic. You can go deeper into the sentences and their topics to find more information:

```{r}
a_doc_topic <- data.frame(topics(model_aus))
head(a_doc_topic, 5) #just showing the beginning of the table
```

We use the corpus on a sentence basis to get a dataframe and merge it with the topics:
```{r}
df_aus_sent <- convert(corpus_sent, to = "data.frame")

a_doc_topic$doc_id <- rownames(a_doc_topic)
aus_topics <- merge(df_aus_sent, a_doc_topic, by= "doc_id")
View(aus_topics)
```

```{r}
env_topic <- aus_topics[which(aus_topics$topics.model_aus.=="10"),]
head(env_topic$text) 

unemp_topic <- aus_topics[which(aus_topics$topics.model_aus.=="15"),]
head(unemp_topic$text) 
```

A closer look at the sentences shows again that it is difficult to expect a specific topic; here, general environmental and work-related discrimination issues are discussed.

Now, we want to know how much the different parties talk about renewable energy and unemployment. We check the length of the topics with nrow and the length of the whole manifesto with the sentences variable in the corpus:

```{r}
summary(corpus_manifestos) #check length

(nrow(env_topic[ env_topic$party == "Die Grünen",])/677)*100
(nrow(env_topic[ env_topic$party == "KPÖ",])/181)*100
(nrow(env_topic[ env_topic$party == "SPÖ",])/2958)*100
(nrow(env_topic[ env_topic$party == "FPÖ",])/289)*100
(nrow(env_topic[env_topic$party == "NEOS",])/1649)*100 
```
As expected, in relation to the manifesto length, the environment/renewable energy topic is most discussed by the Green Party followed by FPÖ and NEOS.

```{r}
(nrow(unemp_topic[unemp_topic$party == "Die Grünen",])/677)*100 
(nrow(unemp_topic[unemp_topic$party == "KPÖ",])/181)*100
(nrow(unemp_topic[unemp_topic$party == "SPÖ",])/2958)*100
(nrow(unemp_topic[ unemp_topic$party == "FPÖ",])/289)*100
(nrow(unemp_topic[unemp_topic$party == "NEOS",])/1649)*100 
```
KPÖ and SPÖ discuss the unemployment topic the most. Be careful with the interpretation and keep the discussed shortcomings in mind.
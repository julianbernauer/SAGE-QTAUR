---
title: "Chapter 7 exercises"
author: "Anna"
date: "2025-01-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1) See my example in PDF format.

2) In Chapter 6, you were asked to create word embeddings. Improve that code based on the new information on word embeddings you learned in this chapter.

For this task we need to load the data and preprocessing from chapter 6, you can open the two files in parallel and load in what you need. We start with the cleaned data here:

WORKS?
```{r}
datadir <- "./data_exercises"
load(file = paste0(datadir,"data_exercises/word2vec_cleandoc.rda"))
```


```{r}
head(clean_doc, n =2)
```

Now we need to make decisions based on the data and our task. Waiting long for our test model to load doesn't sound fun so we use the faster "cbow" algorithm. We don't want to overfit and reduce time efforts but we do have a quite large corpus here, so we go with 50 dimensions initially. The bigger the context window, the more calculations are required. We use a context window of 5 words for cbow, because or text does have some complexity to it. Next, we need to choose the number of iterations, here the default is 5, it is probably a good idea to compare 5, 15, 50 iterations, let us choose 15 initially. Next, we set alpha, which word2vec calls "lr" which we keep at 0.05. We keep negative sampling, and the sampling value as is. 
For minimum occurrence of words we use 3 (min_count). We already removed stopwords in the clean_doc, so we do not add a stopword vector here. Of cause we choose the encoding "UTF-8". 

```{r}
chosen_model <- word2vec(
  clean_doc,
  type = "cbow",
  dim = 50,
  window = 5L,
  iter = 15,
  lr = 0.05,
  hs = FALSE,
  negative = 5L,
  sample = 0.001,
  min_count = 3,
  #  stopwords = character(),
  threads = 1L,
  encoding = "UTF-8"
)
```

You can check the tasks from exercise 6 and see if they changed:
```{r }
set.seed(5)
predict(chosen_model, "ideology", type = "nearest", top_n = 6)

```

Now we have a first model, in a next step we would evaluate this by looking at models with different hyperparameter values. We want to do this initial check based on our task, data and literature knowledge so we don't run an unnecessary amount of models. You will do the evaluation step in exercise 3.

3) In Chapter 6, you were asked to replicate the LDA model based on party manifestos. Use the code from this chapter to tune hyperparameters and evaluate your model. 

We start with the model from chapter 6:

```{r}
dtm_aus
```
We already evaluated k, so we keep 15 topics here. We now want to choose further hyperparameters.
Let us start with one initial model again. Besides our input data and k we need to choose the sampling algorithm
we test both here. For alpha we keep the recommended 0.5 and we look at different burnin and iter values.

Here are models with different hyperparamters we can test:
```{r }
?topicmodels::LDA
set.seed(50)
initial_model <- topicmodels::LDA(dtm_aus, k = 15, method = "Gibbs", control = list(alpha = 0.5, burnin = 10, iter =100))

vem_model <-topicmodels::LDA(dtm_aus, 15, method = "VEM", control = list(alpha = 0.5, burnin = 10, iter =100)) 

ch6_model <- topicmodels::LDA(dtm_aus, k = 15, method = "Gibbs", control = list(alpha = 0.5, burnin = 1000, iter =3000)) 

ch6vem <- topicmodels::LDA(dtm_aus, k = 15, method = "VEM", control = list(alpha = 0.5, burnin = 1000, iter =3000)) 
```

We go deeper than in exercise 2 and evaluate some hyperparameter options:

Choose burnin and iter value based on log-likelihood:
```{r}
set.seed(50)
logLik(initial_model) 
logLik(vem_model)
logLik(ch6_model)
logLik(ch6vem)
```

TO DO DATA
Based on perplexity:
First we need to split the data:
split dtm direktly? WIE
```{r}
set.seed(34)
train_index <- sample(seq_len(ndoc(dfm_fr)), size = 0.8 * ndoc(dfm_fr), replace = FALSE)
dfm_train_fr <- dfm_fr[train_index, ]
dfm_test_fr <- dfm_fr[-train_index, ]
dtm_train_fr <- convert(dfm_train_fr, to = "topicmodels") 
dtm_test_fr <- convert(dfm_test_fr, to = "topicmodels")
```

Now run the model on 80% of the data:
```{r}
set.seed(50)
iter100uk_train <- topicmodels::LDA(dtm_train_uk, k = 40, method = "Gibbs", control = list(alpha = 0.5, burnin = 10, iter =100))
set.seed(50)
iter100fr_train <- topicmodels::LDA(dtm_train_fr, k = 15, method = "Gibbs", control = list(alpha = 0.5, burnin = 10, iter =100)) 
set.seed(50)
iter1000uk_train <- topicmodels::LDA(dtm_train_uk, k = 40, method = "Gibbs", control = list(alpha = 0.5, burnin = 500, iter =1000))
set.seed(50)
iter1000fr_train <- topicmodels::LDA(dtm_train_fr, k = 15, method = "Gibbs", control = list(alpha = 0.5, burnin = 500, iter =1000)) 
set.seed(50)
```

And finally calculate perplexity:
```{r}
topicmodels::perplexity(iter100uk_train, dtm_test_uk) 
topicmodels::perplexity(iter100fr_train, dtm_test_fr) 
topicmodels::perplexity(iter1000uk_train, dtm_test_uk) 
topicmodels::perplexity(iter1000fr_train, dtm_test_fr) 
```

Additionally, we can look at the resulting categories like we did in chapter 6 to see if the results are logical.

Evaluation with the oolong package:
```{r}
library(oolong)
oolong_test <- create_oolong(input_model = model_uk, input_corpus = corpus_uk_sent$text)

```
```{r}
library("shiny")
oolong_test$do_word_intrusion_test()
```
```{r}
oolong_test$lock()
```
```{r}
oolong_test
```


